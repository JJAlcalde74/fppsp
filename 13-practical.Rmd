# Some practical forecasting issues {#practical}

In this final chapter, we address many practical issues that arise in forecasting, and discuss some possible solutions.

## Weekly, daily and sub-daily data {#weekly}

Weekly, daily and sub-daily data can be challenging for forecasting, although for different reasons.

### Weekly data {-}

Weekly data\index{weekly data|(} is difficult to work with because the seasonal period (the number of weeks in a year) is both large and non-integer. The average number of weeks in a year is `r round(365.25/7, 2)`. Most of the methods we have considered require the seasonal period to be an integer. Even if we approximate it by 52, most of the methods will not handle such a large seasonal period efficiently.

The simplest approach is to use an STL\index{STL decomposition} decomposition along with a non-seasonal method applied to the seasonally adjusted data (as discussed in Chapter \@ref(decomposition)). Here is an example using weekly data on US finished motor gasoline products supplied (in millions of barrels per day) from February 1991 to May 2005.

```{r gasstl, fig.cap="Forecasts for weekly US gasoline production using an STL decomposition with an ETS model for the seasonally adjusted data.", fig.asp=0.58}
my_dcmp_spec <- decomposition_model(
  STL(Barrels),
  ETS(season_adjust ~ season("N"))
)
us_gasoline %>%
  model(stl_ets = my_dcmp_spec) %>%
  forecast(h = "2 years") %>%
  autoplot(us_gasoline) +
  labs(y = "Millions of barrels per day",
       title = "Weekly US gasoline production")
```

An alternative approach is to use a dynamic harmonic regression\index{dynamic harmonic regression} model, as discussed in Section \@ref(dhr). In the following example, the number of Fourier\index{Fourier series}\index{AICc} terms was selected by minimising the AICc. The order of the ARIMA model is also selected by minimising the AICc, although that is done within the `ARIMA()` function. We use `PDQ(0,0,0)` to prevent `ARIMA()` trying to handle the seasonality using seasonal ARIMA components.

```{r gasweekly_choosemodel, message=FALSE, include=FALSE}
library(purrr)
model_defs <- map(
  as.list(seq(25)),
  ~ ARIMA(Barrels ~ PDQ(0, 0, 0) + fourier(K = !!.[1]))
)
model_defs <- set_names(model_defs, sprintf("ARIMA + fourier(%i)", seq(25)))
gas_aicc <- numeric(length(model_defs))
for (k in seq_along(model_defs)) {
  fit <- us_gasoline %>%
    model(model_defs[[k]]) %>%
    glance()
  if (!is.null(fit$AICc)) {
    gas_aicc[k] <- fit$AICc
  }
}
bestK <- which.min(gas_aicc)
if (bestK != 6L) {
  stop("Gas DHR model changed")
}
```

```{r gasweekly, message=FALSE}
gas_dhr <- us_gasoline %>%
  model(dhr = ARIMA(Barrels ~ PDQ(0, 0, 0) + fourier(K = 6)))
```

```{r gasarima, message=FALSE, warning=FALSE, include=FALSE, dependson="gasweekly"}
arimaorder <- as.numeric(gas_dhr$dhr[[1]]$fit$spec[1, 1:3])
modelname <- paste0("ARIMA(", arimaorder[1], ",", arimaorder[2], ",", arimaorder[3], ")")
if (!identical(arimaorder, c(0, 1, 1))) {
  stop("Gas DHR ARIMA error changed")
}
```

```{r gasforecast, fig.cap="Forecasts for weekly US gasoline production using a dynamic harmonic regression model.", fig.asp=0.58, dependson="gasweekly"}
gas_dhr %>%
  forecast(h = "2 years") %>%
  autoplot(us_gasoline) +
  labs(y = "Millions of barrels per day",
       title = "Weekly US gasoline production")
```

The fitted model has `r bestK` pairs of Fourier terms and can be written as
$$
  y_t = bt + \sum_{j=1}^{`r bestK`}
    \left[
      \alpha_j\sin\left(\frac{2\pi j t}{52.18}\right) +
      \beta_j\cos\left(\frac{2\pi j t}{52.18}\right)
    \right] +
    \eta_t
$$
where $\eta_t$ is an `r modelname` process. Because $\eta_t$ is non-stationary, the model is actually estimated on the differences of the variables on both sides of this equation. There are `r 2*bestK` parameters to capture the seasonality, while the total number of degrees of freedom is `r NROW(tidy(gas_dhr))` (the other two coming from the MA parameter and the drift parameter).

The STL approach is preferable when the seasonality changes over time. The dynamic harmonic regression approach is preferable if there are covariates that are useful predictors as these can be added as additional regressors.\index{weekly data|)}

### Daily and sub-daily data {-}

Daily\index{daily data} and sub-daily\index{sub-daily data} (such as hourly\index{hourly data}) data are challenging for a different reason --- they often involve multiple seasonal patterns, and so we need to use a method that handles such complex seasonality.\index{multiple seasonality}

Of course, if the time series is relatively short so that only one type of seasonality is present, then it will be possible to use one of the single-seasonal methods we have discussed in previous chapters (e.g., ETS or a seasonal ARIMA model). But when the time series is long enough so that some of the longer seasonal periods become apparent, it will be necessary to use STL, dynamic harmonic regression or Prophet, as discussed in Section \@ref(complexseasonality).\index{STL decomposition}\index{dynamic harmonic regression}

However, these methods only allow for regular seasonality. Capturing seasonality associated with moving events such as Easter, Eid, or the Chinese New Year is more difficult. Even with monthly data, this can be tricky as the festivals can fall in either March or April (for Easter), in January or February (for the Chinese New Year), or at any time of the year (for Eid).\index{Easter}\index{Chinese New Year}

The best way to deal with moving holiday effects is to include dummy variables in the model. This can be done within the `ARIMA()` or `prophet()` functions, for example, but not within `ETS()`. In fact, `prophet()` has a `holiday()` special to easily incorporate holiday effects.\index{prophet@\texttt{prophet()}}

## Time series of counts {#counts}

All\index{count data|(} of the methods discussed in this book assume that the data have a continuous sample space. But often data comes in the form of counts. For example, we may wish to forecast the number of customers who enter a store each day. We could have $0, 1, 2, \dots$, customers, but we cannot have 3.45693 customers.

In practice, this rarely matters provided our counts are sufficiently large. If the minimum number of customers is at least 100, then the difference between a continuous sample space $[100,\infty)$ and the discrete sample space $\{100,101,102,\dots\}$ has no perceivable effect on our forecasts. However, if our data contains small counts $(0, 1, 2, \dots)$, then we need to use forecasting methods that are more appropriate for a sample space of non-negative integers.

Such models are beyond the scope of this book. However, there is one simple method which gets used in this context, that we would like to mention. It is "Croston's method",\index{Croston's method|(} named after its British inventor, John Croston, and first described in @Croston72. Actually, this method does not properly deal with the count nature of the data either, but it is used so often, that it is worth knowing about it.

With Croston's method, we construct two new series from our original time series by noting which time periods contain zero values, and which periods contain non-zero values. Let $q_i$ be the $i$th non-zero quantity, and let $a_i$ be the time between $q_{i-1}$ and $q_i$. Croston's method involves separate simple exponential smoothing forecasts on the two new series $a$ and $q$. Because the method is usually applied to time series of demand for items, $q$ is often called the "demand" and $a$ the "inter-arrival time".

If $\hat{q}_{i+1|i}$ and $\hat{a}_{i+1|i}$ are the one-step forecasts of the $(i+1)$th demand and inter-arrival time respectively, based on data up to demand $i$, then Croston's method gives
\begin{align}
  \hat{q}_{i+1|i} & = (1-\alpha_q)\hat{q}_{i|i-1} + \alpha_q q_i, (\#eq:c2method1)\\
  \hat{a}_{i+1|i} & = (1-\alpha_a)\hat{a}_{i|i-1} + \alpha_a a_i. (\#eq:c2method2)
\end{align}
The smoothing parameters $\alpha_a$ and $\alpha_q$ take values between 0 and 1. Let $j$ be the time for the last observed positive observation. Then the $h$-step ahead forecast for the demand at time $T+h$, is given by the ratio
\begin{equation}\label{c2ratio}
  \hat{y}_{T+h|T} = \hat{q}_{j+1|j}/\hat{a}_{j+1|j}.
\end{equation}
There are no algebraic results allowing us to compute prediction intervals for this method, because the method does not correspond to any statistical model [@SH05].\index{Croston's method}

The `CROSTON()` function\index{CROSTON@\texttt{CROSTON()}} produces forecasts using Croston's method. The two smoothing parameters $\alpha_a$ and $\alpha_q$ are estimated from the data. This is different from the way Croston envisaged the method being used. He would simply use $\alpha_a=\alpha_q=0.1$, and set $a_0$ and $q_0$ to  be equal to the first observation in each of the series.

### Example: Pharmaceutical sales {-}

Figure \@ref(fig:j06) shows the numbers of scripts sold each month for immune sera and immunoglobulin products in Australia. The data contain small counts, with many months registering no sales at all, and only small numbers of items sold in other months.

```{r j06, fig.cap="Numbers of scripts sold for Immune sera and immunoglobulins on the Australian Pharmaceutical Benefits Scheme.", fig.height=3, fig.asp=0.5}
j06 <- PBS %>%
  filter(ATC2 == "J06") %>%
  summarise(Scripts = sum(Scripts))

j06 %>% autoplot(Scripts) +
  labs(y="Number of scripts",
       title = "Sales for immune sera and immunoglobulins")
```

Tables \@ref(tab:j06table) and \@ref(tab:j06table2) shows the first 10 non-zero demand values, with their corresponding inter-arrival times.

```{r j06table, echo=FALSE}
firstten <- j06 %>%
  mutate(
    nonzero = Scripts > 0,
    cumdemand = cumsum(nonzero)
  ) %>%
  filter(cumdemand <= 10) %>%
  select(Month, Scripts)
tab <- firstten %>%
  knitr::kable(
    booktabs = TRUE,
    caption = "The first 10 non-zero demand values."
  )
if (!html) {
  tab <- gsub("\\\\centering","\\\\vspace*{-0.2cm}\\\\centering\\\\small",tab)
  tab <- gsub("\\[t\\]","\\[!ht\\]",tab)
}
tab
```

```{r j06table2, echo=FALSE, warning=FALSE}
options(knitr.kable.NA = "")
q <- firstten$Scripts[firstten$Scripts > 0]
a <- c(NA, diff(which(firstten$Scripts > 0)))
out <- rbind(i = seq(10), q, a)
rownames(out) <- c("$i$", "$q_i$", "$a_i$")
out <- out %>%
  knitr::kable(
    booktabs = TRUE,
    escape = FALSE,
    caption = "The first 10 non-zero demand values shown as  demand and inter-arrival series."
  )
if (!html) {
  out <- gsub("\\\\centering","\\\\vspace*{-0.cm}\\\\centering\\\\small",out)
  out <- gsub("\\[t\\]","\\[!ht\\]",out)
  out <- gsub("\\\\end\\{tabular\\}","\\\\end{tabular}\\\\vspace*{0.2cm}",out)
}
out
lastq <- tail(j06$Scripts[j06$Scripts > 0], 1)
if (tail(j06$Scripts, 1) > 0) {
  lasta <- tail(diff(which(j06$Scripts > 0)), 1)
} else {
  lasta <- NROW(j06) - max(which(j06$Scripts > 0))
}
```

```{r crostonfit, include=FALSE}
fit <- j06 %>%
  model(CROSTON(Scripts))
```

```{r crostoncheck, echo=FALSE}
q0 <- fit[[1]][[1]]$fit$par$estimate[1]
alphaq <- fit[[1]][[1]]$fit$par$estimate[2]
a0 <- fit[[1]][[1]]$fit$par$estimate[3]
alphaa <- fit[[1]][[1]]$fit$par$estimate[4]
q <- j06$Scripts[j06$Scripts > 0]
a <- c(NA, diff(which(j06$Scripts > 0)))
qhat <- alphaq * sum(rev((1 - alphaq)^(seq_along(q) - 1)) * q)
ahat <- alphaa * sum(rev((1 - alphaa)^(seq_along(a) - 1)) * a, na.rm = TRUE)
```

In this example, the smoothing parameters are estimated to be $\alpha_a = `r round(alphaa,2)`$, $\alpha_q = `r round(alphaq,2)`$, $\hat{q}_{1|0}=`r round(q0,2)`$, and $\hat{a}_{1|0}=`r round(a0,2)`$. The final forecasts for the two series are $\hat{q}_{T+1|T} = `r round(qhat,3)`$ and $\hat{a}_{T+1|T} = `r round(ahat,3)`$. So the forecasts are all equal to
$\hat{y}_{T+h|T} = `r round(qhat,3)`/`r round(ahat,3)` = `r round(qhat/ahat,3)`$.

In practice, `fable` does these calculations for you:

```{r crostonfc}
j06 %>%
  model(CROSTON(Scripts)) %>%
  forecast(h = 6)
```

The `Scripts` column repeats the mean rather than provide a full distribution, because there is no underlying stochastic model.

Forecasting models that deal more directly with the count nature of the data, and allow for a forecasting distribution, are described in @christou2015count.\index{count data|)}\index{Croston's method|)}

## Ensuring forecasts stay within limits {#limits}

It is common to want forecasts to be positive, or to require them to be within some specified range $[a,b]$. Both of these situations are relatively easy to handle using transformations.\index{constrained forecasts}\index{forecasting within limits|(}

### Positive forecasts {-}

To impose a positivity constraint,\index{positive forecasts} we can simply work on the log scale. For example, consider the real price of a dozen eggs (1900-1993; in cents) shown in Figure \@ref(fig:positiveeggs). Because of the log transformation, the forecast distributions are constrained to stay positive, and so they will become progressively more skewed as the mean decreases.

```{r positiveeggs, fig.cap="Forecasts for the price of a dozen eggs, constrained to be positive using a log transformation."}
egg_prices <- prices %>% filter(!is.na(eggs))
egg_prices %>%
  model(ETS(log(eggs) ~ trend("A"))) %>%
  forecast(h = 50) %>%
  autoplot(egg_prices) +
  labs(title = "Annual egg prices",
       y = "$US (in cents adjusted for inflation) ")
```


### Forecasts constrained to an interval {-}

To see how to handle data constrained to an interval,\index{forecasting within limits} imagine that the egg prices were constrained to lie within $a=50$ and $b=400$. Then we can transform the data using a scaled logit transform which maps $(a,b)$ to the whole real line:
$$
  y = \log\left(\frac{x-a}{b-x}\right),
$$
where $x$ is on the original scale and $y$ is the transformed data. To reverse the transformation, we will use
$$
  x  = \frac{(b-a)e^y}{1+e^y} + a.
$$
This is not a built-in transformation, so we will need to first setup the transformation functions.

```{r constrained, fig.cap="Forecasts for the price of a dozen eggs, constrained to be lie between 50 and 400 cents US."}
scaled_logit <- function(x, lower = 0, upper = 1) {
  log((x - lower) / (upper - x))
}
inv_scaled_logit <- function(x, lower = 0, upper = 1) {
  (upper - lower) * exp(x) / (1 + exp(x)) + lower
}
my_scaled_logit <- new_transformation(
                    scaled_logit, inv_scaled_logit)
egg_prices %>%
  model(
    ETS(my_scaled_logit(eggs, lower = 50, upper = 400)
          ~ trend("A"))
  ) %>%
  forecast(h = 50) %>%
  autoplot(egg_prices) +
  labs(title = "Annual egg prices",
       y = "$US (in cents adjusted for inflation) ")
```

The bias-adjustment is automatically applied here, and the prediction intervals from these transformations have the same coverage probability as on the transformed scale, because quantiles are preserved under monotonically increasing transformations.

The prediction intervals lie above 50 due to the transformation. As a result of this artificial (and unrealistic) constraint, the forecast distributions have become extremely skewed.\index{forecasting within limits|)}

## Forecast combinations {#combinations}

An easy\index{forecast combinations|(} way to improve forecast accuracy is to use several different methods on the same time series, and to average the resulting forecasts. Over 50 years ago, John Bates and Clive Granger wrote a famous paper [@BatesGranger1969], showing that combining forecasts often leads to better forecast accuracy. Twenty years later, @Clemen89 wrote

>The results have been virtually unanimous: combining multiple forecasts leads to increased forecast accuracy. \dots In many cases one can make dramatic performance improvements by simply averaging the forecasts.

While there has been considerable research on using weighted averages, or some other more complicated combination approach, using a simple average has proven hard to beat.

Here is an example using monthly revenue from take-away food in Australia, from April 1982 to December 2018. We use forecasts from the following models: ETS, STL-ETS, and ARIMA; and we compare the results using the last 5 years (60 months) of observations.

```{r auscafe, message=FALSE, warning=FALSE}
auscafe <- aus_retail %>%
  filter(stringr::str_detect(Industry, "Takeaway")) %>%
  summarise(Turnover = sum(Turnover))
train <- auscafe %>%
  filter(year(Month) <= 2013)
STLF <- decomposition_model(
  STL(log(Turnover) ~ season(window = Inf)),
  ETS(season_adjust ~ season("N"))
)
cafe_models <- train %>%
  model(
    ets = ETS(Turnover),
    stlf = STLF,
    arima = ARIMA(log(Turnover))
  ) %>%
  mutate(combination = (ets + stlf + arima) / 3)
cafe_fc <- cafe_models %>%
  forecast(h = "5 years")
```

Notice that we form a combination in the `mutate()` function by simply taking a linear function of the estimated models. This very simple syntax will automatically handle the forecast distribution appropriately by taking account of the correlation between the forecast errors of the models that are included. However, to keep the next plot simple, we will omit the prediction intervals.

```{r combineplot, dependson="auscafe", fig.cap="Point forecasts from various methods applied to Australian monthly expenditure on eating out."}
cafe_fc %>%
  autoplot(auscafe %>% filter(year(Month) > 2008),
           level = NULL) +
  labs(y = "$ billion",
       title = "Australian monthly expenditure on eating out")
```

```{r combineaccuracy, dependson="auscafe"}
cafe_fc %>%
  accuracy(auscafe) %>%
  arrange(RMSE)
```

ARIMA does particularly well with this series, while the combination approach does even better (based on most measures including RMSE and MAE). For other data, ARIMA may be quite poor, while the combination approach is usually not far off, or better than, the best component method.

### Forecast combination distributions {-}

The `cafe_fc` object contains forecast distributions, from which any prediction interval can usually be computed. Let's look at the intervals for the first period.

```{r cafe_fc_dist}
cafe_fc %>% filter(Month == min(Month))
```

The first three are a mixture of normal and transformed normal distributions. The package does not yet combine such diverse distributions, so the `combination` output is simply the mean instead.

However, if we work with simulated sample paths, it is possible to create forecast distributions for the combination forecast as well.

```{r cafe_fc_gen, warning=FALSE, message=FALSE}
cafe_futures <- cafe_models %>%
  # Generate 1000 future sample paths
  generate(h = "5 years", times = 1000) %>%
  # Compute forecast distributions from future sample paths
  as_tibble() %>%
  group_by(Month, .model) %>%
  summarise(
    dist = distributional::dist_sample(list(.sim))
  ) %>%
  ungroup() %>%
  # Create fable object
  as_fable(index = Month, key = .model,
           distribution = dist, response = "Turnover")
```

\newpage

```{r cafe_fc_gen_futures, warning=FALSE, message=FALSE, dependson="cafe_fc_gen"}
# Forecast distributions for h=1
cafe_futures %>% filter(Month == min(Month))
```

Now all four models, including the combination, are stored as empirical distributions, and we can plot prediction intervals for the combination forecast, as shown in Figure \@ref(fig:auscafecombPI).

```{r auscafecombPI, fig.cap="Prediction intervals for the combination forecast of Australian monthly expenditure on eating out.", fig.asp=0.5}
cafe_futures %>%
  filter(.model == "combination") %>%
  autoplot(auscafe %>% filter(year(Month) > 2008)) +
  labs(y = "$ billion",
       title = "Australian monthly expenditure on eating out")
```

To check the accuracy of the 95% prediction intervals, we can use a Winkler score (defined in Section \@ref(distaccuracy)).

```{r auscafe_winkler}
cafe_futures %>%
  accuracy(auscafe, measures = interval_accuracy_measures,
    level = 95) %>%
  arrange(winkler)
```

Lower is better, so the `combination` forecast is again better than any of the component models.\index{forecast combinations|)}

## Prediction intervals for aggregates {#aggregates}

A common\index{prediction intervals}\index{aggregates} problem is to forecast the aggregate of several time periods of data, using a model fitted to the disaggregated data. For example, we may have monthly data but wish to forecast the total for the next year. Or we may have weekly data, and want to forecast the total for the next four weeks.

If the point forecasts are means, then adding them up will give a good estimate of the total. But prediction intervals are more tricky due to the correlations between forecast errors.

A general solution is to use simulations. Here is an example using ETS models applied to Australian take-away food sales, assuming we wish to forecast the aggregate revenue in the next 12 months.\index{generate@\texttt{generate()}}\index{future sample paths}

```{r aggregates, message=FALSE, dependson="auscafe"}
fit <- auscafe %>%
  # Fit a model to the data
  model(ETS(Turnover))
futures <- fit %>%
  # Simulate 10000 future sample paths, each of length 12
  generate(times = 10000, h = 12) %>%
  # Sum the results for each sample path
  as_tibble() %>%
  group_by(.rep) %>%
  summarise(.sim = sum(.sim)) %>%
  # Store as a distribution
  summarise(total = distributional::dist_sample(list(.sim)))
```

We can compute the mean of the simulations, along with prediction intervals:

```{r aggregates2, dependson="aggregates"}
futures %>%
  mutate(
    mean = mean(total),
    pi80 = hilo(total, 80),
    pi95 = hilo(total, 95)
  )
```

As expected, the mean of the simulated data is close to the sum of the individual forecasts.

```{r aggregates3, dependson="aggregates"}
forecast(fit, h = 12) %>%
  as_tibble() %>%
  summarise(total = sum(.mean))
```

## Backcasting {#backcasting}

Sometimes it is useful to "backcast" a\index{backcasting} time series --- that is, forecast in reverse time. Although there are no in-built R functions to do this, it is easy to implement by creating a new time index.

Suppose we want to extend our Australian takeaway to the start of 1981 (the actual data starts in April 1982).

```{r backcasting, fig.cap="Backcasts for Australian monthly expenditure on cafés, restaurants and takeaway food services using an ETS model.", dependson='auscafe'}
backcasts <- auscafe %>%
  mutate(reverse_time = rev(row_number())) %>%
  update_tsibble(index = reverse_time) %>%
  model(ets = ETS(Turnover ~ season(period = 12))) %>%
  forecast(h = 15) %>%
  mutate(Month = auscafe$Month[1] - (1:15)) %>%
  as_fable(index = Month, response = "Turnover",
    distribution = "Turnover")
backcasts %>%
  autoplot(auscafe %>% filter(year(Month) < 1990)) +
  labs(title = "Backcasts of Australian food expenditure",
       y = "$ (billions)")
```

Most of the work here is in re-indexing the `tsibble` object and then re-indexing the `fable` object.

## Very long and very short time series {#long-short-ts}

### Forecasting very short time series  {-}

We\index{short time series} often get asked how *few* data points can be used to fit a time series model. As with almost all sample size questions, there is no easy answer. It depends on the *number of model parameters to be estimated and the amount of randomness in the data*. The sample size required increases with the number of parameters to be estimated, and the amount of noise in the data.

Some textbooks provide rules-of-thumb giving minimum sample sizes for various time series models. These are misleading and unsubstantiated in theory or practice. Further, they ignore the underlying variability of the data and often overlook the number of parameters to be estimated as well. There is, for example, no justification for the magic number of 30 often given as a minimum for ARIMA modelling. The only theoretical limit is that we need more observations than there are parameters in our forecasting model. However, in practice, we usually need substantially more observations than that.

Ideally, we would test if our chosen model performs well out-of-sample compared to some simpler approaches. However, with short series, there is not enough data to allow some observations to be withheld for testing purposes, and even time series cross validation can be difficult to apply. The AICc\index{AICc} is particularly useful here, because it is a proxy for the one-step forecast out-of-sample MSE. Choosing the model with the minimum AICc value allows both the number of parameters and the amount of noise to be taken into account.\index{short time series}

What tends to happen with short series is that the AICc suggests simple models because anything with more than one or two parameters will produce poor forecasts due to the estimation error. We will fit an ARIMA model to the annual series from the M3-competition with fewer than 20 observations. First we need to create a tsibble, containing the relevant series.

\newpage

```{r shortseries, message=FALSE}
m3totsibble <- function(z) {
  bind_rows(
    as_tsibble(z$x) %>% mutate(Type = "Training"),
    as_tsibble(z$xx) %>% mutate(Type = "Test")
  ) %>%
    mutate(
      st = z$st,
      type = z$type,
      period = z$period,
      description = z$description,
      sn = z$sn
    ) %>%
    as_tibble()
}
short <- Mcomp::M3 %>%
  subset("yearly") %>%
  purrr::map_dfr(m3totsibble) %>%
  group_by(sn) %>%
  mutate(n = max(row_number())) %>%
  filter(n <= 20) %>%
  ungroup() %>%
  as_tsibble(index = index, key = c(sn, period, st))
```

Now we can apply an ARIMA model to each series.

```{r shortfit, dependson="shortseries"}
short_fit <- short %>%
  model(arima = ARIMA(value))
```

```{r shortfit_results, dependson="shortfit", include=FALSE}
nptable <- tidy(short_fit) %>%
  group_by(sn) %>%
  summarise(n = n()) %>%
  right_join(short_fit) %>%
  replace_na(list(n = 0)) %>%
  group_by(n) %>%
  summarise(count = n())
```

Of the `r sum(nptable$count)` series,
`r nptable$count[1L]` had models with zero parameters (white noise and random walks),
`r nptable$count[2L]` had models with one parameter,
`r nptable$count[3L]` had models with two parameters,
`r nptable$count[4L]` had models with three parameters, and only
`r nptable$count[5L]` series had a model with four parameters.

\index{short time series}

### Forecasting very long time series {-}

Most\index{long time series} time series models do not work well for very long time series. The problem is that real data do not come from the models we use. When the number of observations is not large (say up to about 200) the models often work well as an approximation to whatever process generated the data. But eventually we will have enough data that the difference between the true process and the model starts to become more obvious. An additional problem is that the optimisation of the parameters becomes more time consuming because of the number of observations involved.

What to do about these issues depends on the purpose of the model. A more flexible and complicated model could be used, but this still assumes that the model structure will work over the whole period of the data. A better approach is usually to allow the model itself to change over time. ETS models are designed to handle this situation by allowing the trend and seasonal terms to evolve over time. ARIMA models with differencing have a similar property. But dynamic regression models do not allow any evolution of model components.

If we are only interested in forecasting the next few observations, one simple approach is to throw away the earliest observations and only fit a model to the most recent observations. Then an inflexible model can work well because there is not enough time for the relationships to change substantially.

For example, we fitted a dynamic harmonic regression model to 26 years of weekly gasoline production in Section \@ref(weekly). It is, perhaps, unrealistic to assume that the seasonal pattern remains the same over nearly three decades. So we could simply fit a model to the most recent years instead.\index{long time series}

## Forecasting on training and test sets {#training-test}

Typically, we compute one-step forecasts on the training data (the "fitted values") and multi-step forecasts on the test data. However, occasionally we may wish to compute multi-step forecasts on the training data, or one-step forecasts on the test data.\index{training data}\index{test data}

### Multi-step forecasts on training data {-}

We normally define fitted values to be one-step forecasts on the training set (see Section \@ref(residuals)), but a similar idea can be used for multi-step forecasts.  We will illustrate the method using an ARIMA model for the Australian take-away food expenditure. The last five years are used for a test set, and the forecasts are plotted in Figure \@ref(fig:isms).\index{fitted values}\index{training data}

```{r isms, warning=FALSE, fig.cap="Forecasts from an ARIMA model fitted to the Australian monthly expenditure on cafés, restaurants and takeaway food services.", dependson='auscafe', fig.asp=0.58}
training <- auscafe %>% filter(year(Month) <= 2013)
test <- auscafe %>% filter(year(Month) > 2013)
cafe_fit <- training %>%
  model(ARIMA(log(Turnover)))
cafe_fit %>%
  forecast(h = 60) %>%
  autoplot(auscafe) +
  labs(title = "Australian food expenditure",
       y = "$ (billions)")
```

The `fitted()` function\index{fitted@\texttt{fitted()}} has an `h` argument to allow for $h$-step "fitted values" on the training set. Figure \@ref(fig:isms2) is a plot of 12-step (one year) forecasts on the training set. Because the model involves both seasonal (lag 12) and first (lag 1) differencing, it is not possible to compute these forecasts for the first few observations.

```{r isms2, dependson="isms", warning=FALSE, fig.cap="Twelve-step fitted values from an ARIMA model fitted to the Australian café training data.", fig.asp=0.58}
fits12 <- fitted(cafe_fit, h = 12)
training %>%
  autoplot(Turnover) +
  autolayer(fits12, .fitted, col = "#D55E00") +
  labs(title = "Australian food expenditure",
       y = "$ (billions)")
```

\index{fitted values}\index{training data}

### One-step forecasts on test data {-}

It\index{test data} is common practice to fit a model using training data, and then to evaluate its performance on a test data set. The way this is usually done means the comparisons on the test data use different forecast horizons. In the above example, we have used the last sixty observations for the test data, and estimated our forecasting model on the training data. Then the forecast errors will be for 1-step, 2-steps, ..., 60-steps ahead. The forecast variance usually increases with the forecast horizon, so if we are simply averaging the absolute or squared errors from the test set, we are combining results with different variances.

One solution to this issue is to obtain 1-step errors on the test data. That is, we still use the training data to estimate any parameters, but when we compute forecasts on the test data, we use all of the data preceding each observation (both training and test data).  So our training data are for times $1,2,\dots,T-60$. We estimate the model on these data, but then compute $\hat{y}_{T-60+h|T-61+h}$, for $h=1,\dots,T-1$.  Because the test data are not used to estimate the parameters, this still gives us a "fair" forecast.

Using the same ARIMA model used above, we now apply the model to the test data.\index{test data}\index{accuracy@\texttt{accuracy()}}

```{r oosos2, dependson='isms'}
cafe_fit %>%
  refit(test) %>%
  accuracy()
```

Note that model is not re-estimated in this case. Instead, the model obtained previously (and stored as `cafe_fit`) is applied to the `test` data. Because the model was not re-estimated, the "residuals" obtained here are actually one-step forecast errors. Consequently, the results produced from the `accuracy()` command are actually on the test set (despite the output saying "Training set").\index{test data}\index{accuracy@\texttt{accuracy()}} This approach can be used to compare one-step forecasts from different models.

## Dealing with outliers and missing values {#missing-outliers}

Real data often contains missing values, outlying observations, and other messy features. Dealing with them can sometimes be troublesome.

### Outliers {-}

Outliers\index{outliers|(} are observations that are very different from the majority of the observations in the time series. They may be errors, or they may simply be unusual. (See Section \@ref(regression-evaluation) for a discussion of outliers in a regression context.) None of the methods we have considered in this book will work well if there are extreme outliers in the data. In this case, we may wish to replace them with missing values, or with an estimate that is more consistent with the majority of the data.

Simply replacing outliers without thinking about why they have occurred is a dangerous practice. They may provide useful information about the process that produced the data,  which should be taken into account when forecasting. However, if we are willing to assume that the outliers are genuinely errors, or that they won't occur in the forecasting period, then replacing them can make the forecasting task easier.

Figure \@ref(fig:ahoutlier) shows the number of visitors to the Adelaide Hills region of South Australia. There appears to be an unusual observation in 2002 Q4.

```{r ahoutlier, fig.cap="Number of overnight trips to the Adelaide Hills region of South Australia.", fig.asp=0.4}
tourism %>%
  filter(
    Region == "Adelaide Hills", Purpose == "Visiting"
  ) %>%
  autoplot(Trips) +
  labs(title = "Quarterly overnight trips to Adelaide Hills",
       y = "Number of trips")

```

One useful way to find outliers is to apply `STL()` to the series with the argument `robust=TRUE`. Then any outliers should show up in the remainder series. The data in Figure \@ref(fig:ahoutlier) have almost no visible seasonality, so we will apply STL without a seasonal component by setting `period=1`.

```{r stlahdecomp, fig.cap="STL decomposition of visitors to the Adelaide Hills region of South Australia, with no seasonal component.", fig.asp=0.7}
ah_decomp <- tourism %>%
  filter(
    Region == "Adelaide Hills", Purpose == "Visiting"
  ) %>%
  # Fit a non-seasonal STL decomposition
  model(
    stl = STL(Trips ~ season(period = 1), robust = TRUE)
  ) %>%
  components()
ah_decomp %>% autoplot()
```

In the above example the outlier was easy to identify. In more challenging cases, using a boxplot of the remainder series would be useful. We can identify as outliers those that are greater than 1.5 interquartile ranges (IQRs) from the central 50% of the data. If the remainder was normally distributed, this would show 7 in every 1000 observations as "outliers". A stricter rule is to define outliers as those that are greater than 3 interquartile ranges (IQRs) from the central 50% of the data, which would make only 1 in 500,000 normally distributed observations to be outliers. This is the rule we prefer to use.

```{r stl_outliers, dependson="stlahdecomp"}
outliers <- ah_decomp %>%
  filter(
    remainder < quantile(remainder, 0.25) - 3*IQR(remainder) |
    remainder > quantile(remainder, 0.75) + 3*IQR(remainder)
  )
outliers
```

This finds the one outlier that we suspected from Figure \@ref(fig:ahoutlier). Something similar could be applied to the full data set to identify unusual observations in other series.\index{outliers|)}

### Missing values {-}

Missing\index{missing values} data can arise for many reasons, and it is worth considering whether the missingness will induce bias in the forecasting model. For example, suppose we are studying sales data for a store, and missing values occur on public holidays when the store is closed. The following day may have increased sales as a result. If we fail to allow for this in our forecasting model, we will most likely under-estimate sales on the first day after the public holiday, but over-estimate sales on the days after that. One way to deal with this kind of situation is to use a dynamic regression model, with dummy variables indicating if the day is a public holiday\index{holidays} or the day after a public holiday. No automated method can handle such effects as they depend on the specific forecasting context.

In other situations, the missingness may be essentially random. For example, someone may have forgotten to record the sales figures, or the data recording device may have malfunctioned. If the timing of the missing data is not informative for the forecasting problem, then the missing values can be handled more easily.\index{missing values}

Finally, we might remove some unusual observations, thus creating missing values in the series.

Some methods allow for missing values without any problems. For example, the naïve forecasting method continues to work, with the most recent non-missing value providing the forecast for the future time periods. Similarly, the other benchmark methods introduced in Section \@ref(simple-methods) will all produce forecasts when there are missing values present in the historical data. The `fable` functions for ARIMA models, dynamic regression models and NNAR models will also work correctly without causing errors. However, other modelling functions do not handle missing values including `ETS()` and `STL()`.

When missing values cause errors, there are at least two ways to handle the problem. First, we could just take the section of data after the last missing value, assuming there is a long enough series of observations to produce meaningful forecasts. Alternatively, we could replace the missing values with estimates. To do this, we first fit an ARIMA model to the data containing missing values, and then use the model to interpolate the missing observations.\index{interpolation}

We will replace the outlier\index{outliers} identified in Figure \@ref(fig:stlahdecomp) by an estimate using an ARIMA model.

```{r ah_miss, message=FALSE, dependson="stl_outliers"}
ah_miss <- tourism %>%
  filter(
    Region == "Adelaide Hills",
    Purpose == "Visiting"
  ) %>%
  # Remove outlying observations
  anti_join(outliers) %>%
  # Replace with missing values
  fill_gaps()
ah_fill <- ah_miss %>%
  # Fit ARIMA model to the data containing missing values
  model(ARIMA(Trips)) %>%
  # Estimate Trips for all periods
  interpolate(ah_miss)
ah_fill %>%
  # Only show outlying periods
  right_join(outliers %>% select(-Trips))
```

```{r replacment, include=FALSE, dependson="ah_miss"}
outlier <- outliers$Trips
replacement <- ah_miss %>%
  model(ARIMA(Trips)) %>%
  interpolate(ah_miss) %>%
  right_join(outliers %>% select(-Trips)) %>%
  pull(Trips)
```

The `interpolate()` function\index{interpolate@\texttt{interpolate()}} uses the ARIMA model to estimate any missing values in the series. In this case, the outlier of `r round(outlier,1)` has been replaced with `r round(replacement,1)`. The resulting series is shown in Figure \@ref(fig:replacement-plot).

The `ah_fill` data could now be modeled with a function that does not allow missing values.

```{r replacement-plot, fig.cap="Number of overnight trips to the Adelaide Hills region of South Australia with the 2002Q4 outlier being replaced using an ARIMA model for interpolation.", dependson="ah_miss"}
ah_fill %>%
  autoplot(Trips) +
  autolayer(ah_fill %>% filter_index("2002 Q3"~"2003 Q1"),
    Trips, colour="#D55E00") +
  labs(title = "Quarterly overnight trips to Adelaide Hills",
       y = "Number of trips")
```

\index{interpolation}

## Further reading

So many diverse topics are discussed in this chapter, that it is not possible to point to specific references on all of them. The last chapter in @Ord2017 also covers "Forecasting in practice" and discusses other issues that might be of interest to readers.
