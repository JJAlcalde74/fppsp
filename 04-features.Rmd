# Time series features {#features}

The `feasts` package\index{feasts package@\texttt{feasts} package} includes functions for computing FEatures And Statistics from Time Series (hence the name).\index{time series features|(} We have already seen some time series features. For example, the autocorrelations discussed in Section \@ref(acf) can be considered features of a time series --- they are numerical summaries computed from the series. Another feature we saw in the last chapter was the Guerrero estimate of the Box-Cox transformation parameter --- again, this is a number computed from a time series.

We can compute many different features on many different time series, and use them to explore the properties of the series. In this chapter we will look at some features that have been found useful in time series exploration, and how they can be used to uncover interesting information about your data. We will use Australian quarterly tourism as a running example (previously discussed in Section \@ref(subseries)).

## Some simple statistics

Any numerical summary computed from a time series is a feature of that time series --- the mean, minimum or maximum, for example. These can be computed using the `features()` function.\index{features@\texttt{features()}} For example, let's compute the means of all the series in the Australian tourism data.

\newpage

```{r feature_mean2}
tourism %>%
  features(Trips, list(mean = mean)) %>%
  arrange(mean)
```

Here we see that the series with least average number of visits was "Other" visits to Kangaroo Island in South Australia.

Rather than compute one feature at a time, it is convenient to compute many features at once. A common short summary of a data set is to compute five summary statistics: the minimum, first quartile, median, third quartile and maximum. These divide the data into four equal-size sections, each containing 25% of the data. The `quantile()` function\index{quantile@\texttt{quantile()}} can be used to compute them.

```{r feature_fivenum}
tourism %>% features(Trips, quantile)
```

Here the minimum is labelled `0%` and the maximum is labelled `100%`.

## ACF features

Autocorrelations were discussed in Section \@ref(acf).\index{autocorrelation|(} All the autocorrelations of a series can be considered features of that series. We can also summarise the autocorrelations to produce new features; for example, the sum of the first ten squared autocorrelation coefficients is a useful summary of how much autocorrelation there is in a series, regardless of lag.

We can also compute autocorrelations of the changes in the series between periods. That is, we "difference"\index{differencing} the data and create a new time series consisting of the differences between consecutive observations. Then we can compute the autocorrelations of this new differenced series. Occasionally it is useful to apply the same differencing operation again, so we compute the differences of the differences. The autocorrelations of this double differenced series may provide useful information.

Another related approach is to compute seasonal differences\index{seasonal differencing} of a series. If we had monthly data, for example, we would compute the difference between consecutive Januaries, consecutive Februaries, and so on. This enables us to look at how the series is changing between years, rather than between months. Again, the autocorrelations of the seasonally differenced series may provide useful information.

We discuss differencing of time series in more detail in Section \@ref(stationarity).

The `feat_acf()` function\index{feat_acf@\texttt{feat\_acf()}} computes a selection of the autocorrelations discussed here. It will return six or seven features:

 * the first autocorrelation coefficient from the original data;
 * the sum of squares of the first ten autocorrelation coefficients from the original data;
 * the first autocorrelation coefficient from the differenced data;
 * the sum of squares of the first ten autocorrelation coefficients from the differenced data;
 * the first autocorrelation coefficient from the twice differenced data;
 * the sum of squares of the first ten autocorrelation coefficients from the twice differenced data;
 * For seasonal data, the autocorrelation coefficient at the first seasonal lag is also returned.

When applied to the Australian tourism data, we get the following output.

```{r feature_acf}
tourism %>% features(Trips, feat_acf)
```

\index{autocorrelation|)}

## STL Features {#stlfeatures}

The STL decomposition discussed in Chapter \@ref(decomposition) is the basis for several more features.\index{STL decomposition}

A time series decomposition can be used to measure the strength of trend and seasonality\index{strength of seasonality}\index{strength of trend} in a time series. Recall that the decomposition is written as
$$
  y_t = T_t + S_{t} + R_t,
$$
where $T_t$ is the smoothed trend component, $S_{t}$ is the seasonal component and $R_t$ is a remainder component. For strongly trended data, the seasonally adjusted data should have much more variation than the remainder component. Therefore Var$(R_t)$/Var$(T_t+R_t)$ should be relatively small. But for data with little or no trend, the two variances should be approximately the same. So we define the strength of trend as:\index{strength of trend}\index{trend}
$$
  F_T = \max\left(0, 1 - \frac{\text{Var}(R_t)}{\text{Var}(T_t+R_t)}\right).
$$
This will give a measure of the strength of the trend between 0 and 1. Because the variance of the remainder might occasionally be even larger than the variance of the seasonally adjusted data, we set the minimal possible value of $F_T$ equal to zero.

The strength of seasonality is defined similarly,\index{strength of seasonality}\index{seasonality} but with respect to the detrended data rather than the seasonally adjusted data:
$$
  F_S = \max\left(0, 1 - \frac{\text{Var}(R_t)}{\text{Var}(S_{t}+R_t)}\right).
$$
A series with seasonal strength $F_S$ close to 0 exhibits almost no seasonality, while a series with strong seasonality will have $F_S$ close to 1 because Var$(R_t)$ will be much smaller than Var$(S_{t}+R_t)$.

These measures can be useful, for example, when you have a large collection of time series, and you need to find the series with the most trend or the most seasonality. These and other STL-based features are computed using the `feat_stl()` function.\index{feat_stl@\texttt{feat\_stl()}}

```{r stl-features, echo = TRUE}
tourism %>%
  features(Trips, feat_stl)
```

We can then use these features in plots to identify what type of series are heavily trended and what are most seasonal.

```{r featuresplot, fig.height=4.6, fig.cap="Seasonal strength vs trend strength for all tourism series.", fig.env="figure*"}
tourism %>%
  features(Trips, feat_stl) %>%
  ggplot(aes(x = trend_strength, y = seasonal_strength_year,
             col = Purpose)) +
  geom_point() +
  facet_wrap(vars(State))
```

Clearly, holiday series are most seasonal which is unsurprising. The strongest trends tend to be in Western Australia and Victoria. The most seasonal series can also be easily identified and plotted.

```{r extreme, fig.height=3, fig.asp=0.45, fig.cap="The most seasonal series in the Australian tourism data."}
tourism %>%
  features(Trips, feat_stl) %>%
  filter(
    seasonal_strength_year == max(seasonal_strength_year)
  ) %>%
  left_join(tourism, by = c("State", "Region", "Purpose")) %>%
  ggplot(aes(x = Quarter, y = Trips)) +
  geom_line() +
  facet_grid(vars(State, Region, Purpose))
```

This shows holiday trips to the most popular ski region of Australia.

The `feat_stl()` function\index{feat_stl@\texttt{feat\_stl()}} returns several more features other than those discussed above.

 * `seasonal_peak_year` indicates the timing of the peaks --- which month or quarter contains the largest seasonal component. This tells us something about the nature of the seasonality. In the Australian tourism data, if Quarter 3 is the peak seasonal period, then people are travelling to the region in winter, whereas a peak in Quarter 1 suggests that the region is more popular in summer.
 * `seasonal_trough_year` indicates the timing of the troughs --- which month or quarter contains the smallest seasonal component.
 * `spikiness` measures the prevalence of spikes in the remainder component $R_t$ of the STL decomposition. It is the variance of the leave-one-out variances of $R_t$.
 * `linearity` measures the linearity of the trend component of the STL decomposition. It is based on the coefficient of a linear regression applied to the trend component.
 * `curvature` measures the curvature of the trend component of the STL decomposition. It is based on the coefficient from an orthogonal quadratic regression applied to the trend component.
 * `stl_e_acf1` is the first autocorrelation coefficient of the remainder series.
 * `stl_e_acf10` is the sum of squares of the first ten autocorrelation coefficients of the remainder series.

## Other features

Many more features are possible, and the `feasts` package\index{feasts package@\texttt{feasts} package|(} computes only a few dozen features that have proven useful in time series analysis. It is also easy to add your own features by writing an R function that takes a univariate time series input and returns a numerical vector containing the feature values.

The remaining features in the `feasts` package, not previously discussed, are listed here for reference. The details of some of them are discussed later in the book.

 * `coef_hurst` will calculate the Hurst coefficient\index{Hurst coefficient} of a time series which is a measure of "long memory".\index{long memory} A series with long memory will have significant autocorrelations for many lags.\index{coef_hurst@\texttt{coef\_hurst()}}
 * `feat_spectral` will compute the (Shannon) spectral entropy\index{spectral entropy} of a time series, which is a measure of how easy the series is to forecast. A series which has strong trend and seasonality (and so is easy to forecast) will have entropy close to 0. A series that is very noisy (and so is difficult to forecast) will have entropy close to 1.\index{feat_spectral@\texttt{feat\_spectral()}}
 * `box_pierce` gives the Box-Pierce statistic for testing if a time series is white noise, and the corresponding p-value. This test is discussed in Section \@ref(diagnostics).\index{Box-Pierce test}\index{box_pierce@\texttt{box\_pierce()}}
 * `ljung_box` gives the Ljung-Box statistic for testing if a time series is white noise, and the corresponding p-value. This test is discussed in Section \@ref(diagnostics).\index{Ljung-Box test}\index{ljung_box@\texttt{ljung\_box()}}
 * The $k$th partial autocorrelation\index{partial autocorrelation} measures the relationship between observations $k$ periods apart after removing the effects of observations between them. So the first partial autocorrelation ($k=1$) is identical to the first autocorrelation, because there is nothing between consecutive observations to remove. Partial autocorrelations are discussed in Section \@ref(non-seasonal-arima). The `feat_pacf` function\index{feat_pacf@\texttt{feat\_pacf()}} contains several features involving partial autocorrelations including the sum of squares of the first five partial autocorrelations for the original series, the first-differenced series and the second-differenced series. For seasonal data, it also includes the partial autocorrelation at the first seasonal lag.
 * `unitroot_kpss` gives the Kwiatkowski-Phillips-Schmidt-Shin (KPSS)\index{KPSS test} statistic for testing if a series is stationary, and the corresponding p-value. This test is discussed in Section \@ref(stationarity).\index{unitroot_kpss@\texttt{unitroot\_kpss()}}
 * `unitroot_pp` gives the Phillips-Perron statistic\index{Phillips-Perron test} for testing if a series is non-stationary, and the corresponding p-value.\index{unitroot_pp@\texttt{unitroot\_pp()}}
 * `unitroot_ndiffs` gives the number of differences required to lead to a stationary series based on the KPSS test. This is discussed in Section \@ref(stationarity)\index{unitroot_ndiffs@\texttt{unitroot\_ndiffs()}}
 * `unitroot_nsdiffs` gives the number of seasonal differences required to make a series stationary. This is discussed in Section \@ref(stationarity).\index{unitroot_nsdiffs@\texttt{unitroot\_nsdiffs()}}
 * `var_tiled_mean` gives the variances of the "tiled means" (i.e., the means of consecutive non-overlapping blocks of observations). The default tile length is either 10 (for non-seasonal data) or the length of the seasonal period. This is sometimes called the "stability" feature.
 * `var_tiled_var` gives the variances of the "tiled variances" (i.e., the variances of consecutive non-overlapping blocks of observations). This is sometimes called the "lumpiness" feature.
 * `shift_level_max` finds the largest mean shift between two consecutive sliding windows of the time series. This is useful for finding sudden jumps or drops in a time series.
 * `shift_level_index` gives the index at which the largest mean shift occurs.
 * `shift_var_max` finds the largest variance shift between two consecutive sliding windows of the time series. This is useful for finding sudden changes in the volatility of a time series.
 * `shift_var_index` gives the index at which the largest variance shift occurs.
 * `shift_kl_max` finds the largest distributional shift (based on the Kulback-Leibler divergence) between two consecutive sliding windows of the time series. This is useful for finding sudden changes in the distribution of a time series.
 * `shift_kl_index` gives the index at which the largest KL shift occurs.
 * `n_crossing_points` computes the number of times a time series crosses the median.
 * `longest_flat_spot` computes the number of sections of the data where the series is relatively unchanging.
 * `stat_arch_lm` returns the statistic based on the Lagrange Multiplier (LM) test of Engle (1982) for autoregressive conditional heteroscedasticity (ARCH).
 * `guerrero` computes the optimal $\lambda$ value for a Box-Cox transformation using the Guerrero method (discussed in Section \@ref(transformations)).\index{Guerrero transformation}\index{guerrero@\texttt{guerrero()}}

\index{feasts package@\texttt{feasts} package|)}

## Exploring Australian tourism data

All of the features included in the `feasts` package can be computed in one line like this.\index{feasts package@\texttt{feasts} package}\index{feature_set@\texttt{feature\_set()}}

```{r all_features, warning=FALSE}
tourism_features <- tourism %>%
  features(Trips, feature_set(pkgs = "feasts"))
tourism_features
```

This gives `r NCOL(tourism_features)-3` features for every combination of the three key variables (`Region`, `State` and `Purpose`). We can treat this tibble like any data set and analyse it to find interesting observations or groups of observations.

We've already seen how we can plot one feature against another (Section \@ref(stlfeatures)). We can also do pairwise plots of groups of features. In Figure \@ref(fig:seasonalfeatures), for example, we show all features that involve seasonality, along with the `Purpose` variable.

```{r seasonalfeatures, fig.cap="Pairwise plots of all the seasonal features for the Australian tourism data", message=FALSE, fig.width=12, fig.height=12, fig.asp=1, fig.env = 'figure*'}
library(glue)
tourism_features %>%
  select_at(vars(contains("season"), Purpose)) %>%
  mutate(
    seasonal_peak_year = seasonal_peak_year +
      4*(seasonal_peak_year==0),
    seasonal_trough_year = seasonal_trough_year +
      4*(seasonal_trough_year==0),
    seasonal_peak_year = glue("Q{seasonal_peak_year}"),
    seasonal_trough_year = glue("Q{seasonal_trough_year}"),
  ) %>%
  GGally::ggpairs(mapping = aes(colour = Purpose))
```

\index{ggpairs@\texttt{ggpairs()}}

Here, the `Purpose` variable is mapped to colour. There is a lot of information in this figure, and we will highlight just a few things we can learn.

  * The three numerical measures related to seasonality (`seasonal_strength_year`, `season_acf1` and `season_pacf`) are all positively correlated.
  * The bottom left panel and the top right panel both show that the most strongly seasonal series are related to holidays (as we saw previously).
  * The bar plots in the bottom row of the `seasonal_peak_year` and `seasonal_trough_year` columns show that seasonal peaks in Business travel occur most often in Quarter 3, and least often in Quarter 1.

It is difficult to explore more than a handful of variables in this way. A useful way to handle many more variables is to use a dimension reduction technique such as principal components. This gives linear combinations of variables that explain the most variation in the original data. We can compute the principal components of the tourism features as follows.

(ref:pca) A plot of the first two principal components, calculated from the `r NCOL(tourism_features)-3` features of the Australian quarterly tourism data.

```{r pca, fig.cap="(ref:pca)", out.width="70%", fig.width=4, fig.height=4, fig.asp=1}
library(broom)
pcs <- tourism_features %>%
  select(-State, -Region, -Purpose) %>%
  prcomp(scale = TRUE) %>%
  augment(tourism_features)
pcs %>%
  ggplot(aes(x = .fittedPC1, y = .fittedPC2, col = Purpose)) +
  geom_point() +
  theme(aspect.ratio = 1)
```

Each point on Figure \@ref(fig:pca) represents one series and its location on the plot is based on all `r NCOL(tourism_features)-3` features. The first\index{principal components} principal component (`.fittedPC1`) is the linear combination of the features which explains the most variation in the data. The second principal component (`.fittedPC2`) is the linear combination which explains the next most variation in the data, while being uncorrelated with the first principal component. For more information about principal component dimension reduction, see @izenman2008.

Figure \@ref(fig:pca) reveals a few things about the tourism data. First, the holiday series behave quite differently from the rest of the series. Almost all of the holiday series appear in the top half of the plot, while almost all of the remaining series appear in the bottom half of the plot. Clearly, the second principal component is distinguishing between holidays and other types of travel.

The plot also allows us to identify anomalous time series --- series which\index{anomalous time series} have unusual feature combinations. These appear as points that are separate from the majority of series in Figure \@ref(fig:pca). There are four that stand out, and we can identify which series they correspond to as follows.

```{r pcaoutliers, dependson='pca', fig.asp=1, fig.cap="Four anomalous time series from the Australian tourism data."}
outliers <- pcs %>%
  filter(.fittedPC1 > 10) %>%
  select(Region, State, Purpose, .fittedPC1, .fittedPC2)
outliers
outliers %>%
  left_join(tourism, by = c("State", "Region", "Purpose")) %>%
  mutate(
    Series = glue("{State}", "{Region}", "{Purpose}",
                  .sep = "\n\n")
  ) %>%
  ggplot(aes(x = Quarter, y = Trips)) +
  geom_line() +
  facet_grid(Series ~ ., scales = "free") +
  labs(title = "Outlying time series in PC space")
```

\newpage

We can speculate why these series are identified as unusual.

 * Holiday visits to the south coast of NSW is highly seasonal but has almost no trend, whereas most holiday destinations in Australia show some trend over time.
 * Melbourne is an unusual holiday destination because it has almost no seasonality, whereas most holiday destinations in Australia have highly seasonal tourism.
 * The north western corner of Western Australia is unusual because it shows an increase in business tourism in the last few years of data, but little or no seasonality.
 * The south western corner of Western Australia is unusual because it shows both an increase in holiday tourism in the last few years of data and a high level of seasonality.

\newpage

## Exercises {#feast-exercises}

1. Write a function to compute the mean and standard deviation of a time series, and apply it to the `PBS` data. Plot the series with the highest mean, and the series with the lowest standard deviation.

2. Use `GGally::ggpairs()` to look at the relationships between the STL-based features for the holiday series in the `tourism` data. Change `seasonal_peak_year` and `seasonal_trough_year` to factors, as shown in Figure \@ref(fig:seasonalfeatures). Which is the peak quarter for holidays in each state?

3. Use a feature-based approach to look for outlying series in the `PBS` data. What is unusual about the series you identify as "outliers".

## Further reading

 * The idea of using STL for features originated with @WangSH06.
 * The features provided by the `feasts` package were motivated by their use in @cikm2015 and @m3pca.
 * The exploration of a set of time series using principal components on a large collection of features was proposed by @m3pca.

\index{time series features|)}
